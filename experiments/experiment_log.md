# Experiment Log

This file tracks experiment results, meeting notes, and research directions.

---

## 2026-02-06: Meeting Notes

### Current Results Discussion
- Velocity experiments showing interesting disambiguation effects
- Early disambiguation (10%) shows strongest velocity drop (~12%)
- Effect builds progressively through layers, clearest in final 5-7 layers

### Methodological Considerations

**Metrics to revisit:**
- Curse of dimensionality may affect distance-based metrics
- **Action**: Use cosine similarity instead of Euclidean distance for cluster preference experiments
- Note: Some tokens have same connectivity patterns

**Open questions:**
- Do in-context representations change at the last layer?
- If only one hypothesis exists, what does velocity look like?
- Need to find better y-axis metrics to understand what's happening

### Research Directions

#### 1. Representational Collapse (PRIMARY FOCUS)
- Investigate when/why collapse happens
- Related concepts to explore:
  - "Echo chamber" effects
  - Can collapse be reverted?
- **Experiments to run:**
  - Random walk on one hypothesis → observe collapse
  - Then random walk on other hypothesis → does it recover?
  - Reproduce ring structure and check for collapse

#### 2. Performance Linkage
- Track if representational collapse correlates with task performance
- Important for practical implications

#### 3. Semantic Knowledge Conflicts
- Test if existing semantic knowledge affects collapse
- Hypothesis: Conflict level impacts representational collapse severity
- Current experiments use random token sampling → high variance
- Need controlled experiments with varying conflict levels

#### 4. Natural Language Validation
- Test if velocity/collapse effects replicate with natural language prompts
- Important for generalization beyond synthetic graphs

#### 5. Pretraining vs Fine-tuning Dynamics
- Literature suggests: ICL can override SFT but not pretraining
- Interesting to show ICL can override hierarchical representations
- Compare: Pretraining vs SFT vs RLHF effects

### Context from Tommy
- "Context rot" is a known problem in practice
- Effective context length often only ~25% of stated length
- Our findings may relate to this degradation

### Literature Review Needed
- Representational collapse in LLMs
- Echo chamber effects in neural networks
- Context length degradation / context rot

### Immediate Next Steps
1. Look at representational collapse - when does it happen?
2. Find different y-axis metrics (beyond raw velocity)
3. Literature review on collapse phenomena

---

## 2026-02-06: Literature Review - Collapse Phenomena

### 1. Model Collapse (Training on Synthetic Data)

**Key Paper**: [AI models collapse when trained on recursively generated data](https://www.nature.com/articles/s41586-024-07566-y) (Nature, July 2024)

**Definition**: When models are trained on data generated by previous model versions, irreversible defects emerge where tails of the original distribution disappear.

**Stages**:
- **Early collapse**: Tails of distribution disappear (rare events vanish first)
- **Late collapse**: Model converges to shrunken distribution with very low variance

**Key findings**:
- Even 0.1% synthetic data can trigger collapse ([Strong Model Collapse, ICLR 2025](https://openreview.net/forum?id=et5l9qPUhm))
- Larger models can *amplify* collapse, not mitigate it
- By April 2025, ~30-40% of web corpus is synthetic
- Results in "knowledge collapse" - regression to mainstream/central tendencies

**Relevance to our work**: Our velocity collapse after disambiguation may relate to the model converging to a lower-variance representation state.

---

### 2. Neural Collapse (Classification Networks)

**Key Paper**: [Prevalence of neural collapse during terminal phase of training](https://www.pnas.org/doi/10.1073/pnas.2015509117) (PNAS)

**Definition**: Simplification in learned representations during late training stages (Terminal Phase of Training, TPT).

**Four manifestations (NC1-NC4)**:
1. **NC1 - Variability collapse**: Within-class variation becomes negligible, activations collapse to class means
2. **NC2 - Simplex ETF**: Class means converge to equal-length vectors with maximally equal angles
3. **NC3 - Self-duality**: Linear classifiers approach their corresponding class means
4. **NC4 - Simplified decisions**: Nearest-class-center becomes optimal classifier

**Benefits**: Convergence to this structure correlates with improved generalization and adversarial robustness.

**Extended findings**:
- [Neural collapse in intermediate layers](https://arxiv.org/abs/2308.02760) - some degree emerges in most hidden layers
- [Generalized neural collapse](https://arxiv.org/abs/2310.05351) - extends to cases where #classes >> feature dimension (relevant for LLMs)

**Relevance to our work**: Our late-layer velocity drop may indicate neural collapse-like convergence to simplified representations.

---

### 3. Context Rot / Lost in the Middle

**Key Paper**: [Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/abs/2307.03172) (Stanford, 2023)

**Definition**: Performance degradation as context length increases, even for simple tasks.

**Key findings** ([Chroma Research](https://research.trychroma.com/context-rot)):
- With 20 retrieved documents (~4K tokens), accuracy drops from 70-75% to 55-60%
- U-shaped attention bias: beginning and end tokens receive higher attention
- Effective context length often only ~25% of stated maximum
- [RoPE introduces long-term decay](https://arxiv.org/abs/2406.16008) that deprioritizes middle content

**Solutions proposed**:
- [Found in the Middle](https://arxiv.org/abs/2406.16008): Calibration mechanism for faithful attention
- Strategic document ordering (important info at start/end)
- Multi-scale Positional Encoding (Ms-PoE)

**Relevance to our work**: Our velocity patterns across context length may reflect attention distribution changes. The ~25% effective length aligns with Tommy's observation.

---

### 4. Representation Degeneration / Anisotropy

**Key Papers**:
- [Anisotropy Is Inherent to Self-Attention](https://arxiv.org/html/2401.12143v2)
- [Anisotropy is Not Inherent to Transformers](https://aclanthology.org/2024.naacl-long.274/) (NAACL 2024)

**Definition**: Hidden representations become unexpectedly close in angular distance (high cosine similarity), reducing expressiveness.

**Causes**:
- Long-tailed token distributions push representations toward specific directions
- Cross-entropy loss optimization on rare tokens
- Unused/rare tokens push all representations away from origin

**Architecture differences**:
- **Encoders**: Uniformly distributed anisotropy across layers
- **Decoders**: Bell-shaped curve with highest anisotropy in middle layers

**Training dynamics** ([Shape of Learning](https://arxiv.org/abs/2311.05928)):
- Intrinsic dimension increases early (expansion phase)
- Then decreases late in training (compression phase)

**Relevance to our work**: High-dimensional distance metrics may be affected. Cosine similarity recommended (aligns with meeting note).

---

### 5. Layer-wise Representation Dynamics

**Key Paper**: [Layer by Layer: Uncovering Hidden Representations](https://arxiv.org/html/2502.02013) (2025)

**Findings**:
- Early layers: High similarity across models
- Middle layers: Diverse, task-specific patterns
- Later layers: Convergence again (for prediction)

**Compression patterns**:
- Larger models show more pronounced intermediate compression (entropy dips)
- Performance often peaks in middle layers where compression preserves semantic details
- Smoother token trajectories in larger models

**Concept depth** ([Exploring Concept Depth](https://arxiv.org/html/2404.07066v1)):
- Emotional concepts: accuracy rises in initial layers, converges in intermediate
- Complex tasks: convergence at significantly deeper layers
- Different model families converge at different layer depths

**Relevance to our work**: Our layer-wise velocity patterns (effect building through layers 20-27) may reflect this convergence dynamic.

---

### 6. In-Context Learning Representation Theory

**Key Paper**: [In-Context Learning with Representations](https://arxiv.org/abs/2408.10147) (NeurIPS 2024)

**Findings**:
- Transformers learn to perform ridge regression over basis functions
- MLPs act as common nonlinear feature maps, enhancing ICL power
- Training loss converges linearly to global minimum
- First proof that transformers learn contextual information for generalization

**Dynamics**: Mean-field analysis shows loss landscape becomes benign, gradient flow avoids saddle points.

**Relevance to our work**: Theoretical grounding for how in-context examples shape representations.

---

### Summary Table: Collapse Phenomena

| Phenomenon | Domain | Mechanism | Reversible? | Our Connection |
|------------|--------|-----------|-------------|----------------|
| Model Collapse | Training | Synthetic data recursion | No (without real data) | Velocity reduction may be analogous |
| Neural Collapse | Classification | Late-stage training | N/A (training outcome) | Late-layer convergence patterns |
| Context Rot | Inference | Attention dilution | Partially (with calibration) | Context length effects |
| Anisotropy | Embeddings | Token distribution | Yes (with techniques) | Use cosine similarity |
| Layer Convergence | Deep networks | Compression for prediction | N/A (architectural) | Layer-wise velocity patterns |

---

### Open Questions for Our Research

1. **Is our velocity drop a form of "inference-time neural collapse"?**
   - Representations converging to simplified structure after disambiguation

2. **Can we measure anisotropy changes during context processing?**
   - Track isotropy/anisotropy as context grows

3. **Does our collapse correlate with context rot?**
   - Test if disambiguation helps or hurts downstream task performance

4. **Can collapse be reversed by conflicting context?**
   - After single-hypothesis collapse, introduce contradicting hypothesis

5. **How does this interact with the "lost in the middle" phenomenon?**
   - Is disambiguation position effect related to attention bias?

---

## Research Task: Anisotropy Tracking

### Status: IN PROGRESS (via collapse experiment)

### Objective
Measure how anisotropy changes as context length grows to understand if "context rot" is fundamentally an anisotropy problem.

### Connection to Current Work
Our `avg_cos_sim` metric in the collapse experiment directly measures anisotropy:
- `avg_cos_sim → 1.0` = high anisotropy = representations collapsed to narrow cone
- `avg_cos_sim → 0.0` = isotropic = healthy uniform spread

### Literature
See `docs/literature/anisotropy_references.md` for full references.

Key papers:
1. **Anisotropy Is Inherent to Self-Attention** (2024) - https://arxiv.org/abs/2401.12143
2. **Anisotropy is Not Inherent to Transformers** (NAACL 2024) - https://aclanthology.org/2024.naacl-long.274/
3. **Shape of Learning** (2023) - https://arxiv.org/abs/2311.05928

### Key Findings from Literature
- Decoders show bell-shaped anisotropy curve (highest in middle layers)
- Training dynamics: expansion phase → compression phase
- Long-tailed token distributions push representations toward specific directions

### Analysis Plan (after collapse experiment completes)
1. Plot `avg_cos_sim` vs context length for all conditions
2. Compare anisotropy patterns: structured vs natural language vs vocabulary size
3. Check if disambiguation reduces anisotropy (restores isotropy)
4. Examine layer-wise anisotropy patterns (expect bell-shape in middle layers)

### Hypotheses
- H1: Anisotropy increases monotonically with context length (context rot)
- H2: Disambiguation temporarily reduces anisotropy before it increases again
- H3: Natural language shows different anisotropy trajectory than structured walks

---

## 2026-02-06: Cluster Preference Metric Comparison

### Objective
Compare distance metrics for cluster preference computation to address curse of dimensionality concerns.

### Metrics Tested
1. **L2 distance** (baseline) - Euclidean distance in full 3584-dim space
2. **Cosine similarity** - Angular distance, scale-invariant
3. **PCA + L2** - Dimensionality reduction to 10, 25, 50, 100 dims, then L2

### Results (50 trials, Qwen2.5-7B, 200 tokens)

| Metric | Layer 0 | Layer 27 | Variance Pattern |
|--------|---------|----------|------------------|
| L2 | 0.531 | 0.537 | High in mid-layers (0.075) |
| **Cosine** | **0.545** | **0.559** | **Consistently low (0.008-0.026)** |
| PCA-10 | 0.543 | 0.551 | Medium |
| PCA-25 | 0.535 | 0.542 | Medium |

### Key Findings
1. **Cosine similarity is recommended** - highest signal, lowest variance
2. **L2 suffers from curse of dimensionality** - high variance in middle layers
3. **PCA-10 is a good alternative** - nearly matches cosine with computational benefits
4. All metrics show H1 preference >0.5 (model encodes graph structure)
5. Layer 27 shows strongest cluster preference (final layer convergence)

### Implications
- Switch to cosine similarity for future cluster preference experiments
- The high L2 variance in layers 7-21 explains why previous experiments showed weak signals
- Low-dimensional PCA projection preserves discriminative structure

### Plots
- `results/cluster_preference_comparison/preference_by_metric.png`
- `results/cluster_preference_comparison/preference_heatmap.png`

---

## W&B Runs

| Date | Run Name | URL | Description |
|------|----------|-----|-------------|
| 2026-02-06 | velocity-scaled-20trials | [link](https://wandb.ai/thomasjiralerspong/icl-structural-influence/runs/17rtiyzh) | 20 trials, 28 layers, 1000 tokens, 8 disambiguation conditions |
| 2026-02-06 | cluster-preference-metric-comparison | [link](https://wandb.ai/thomasjiralerspong/icl-structural-influence/runs/y9te13mp) | 50 trials comparing L2/cosine/PCA metrics |
| 2026-02-06 | preference-disambiguation-cosine | [link](https://wandb.ai/thomasjiralerspong/icl-structural-influence/runs/akzijj9m) | 50 trials, disambiguation effects with cosine similarity |
| 2026-02-06 | preference-dual-interp | [link](https://wandb.ai/thomasjiralerspong/icl-structural-influence/runs/prr23b4v) | 50 trials, proper dual-interpretation graph with ~60% agreement |

---

## 2026-02-06: Preference Disambiguation Experiment

### Objective
Track how cluster preference (cosine similarity) evolves with disambiguation.

### Setup
- H1: Graph cluster labels (semantic structure)
- H2: Position-based labels (i % 3) - control/baseline
- Conditions: 25%, 50%, 75% disambiguation, and none

### Results

| Condition | Layer 27 Final Pref | Preference Change |
|-----------|--------------------|--------------------|
| 25pct | 0.481 | -0.019 |
| 50pct | 0.495 | -0.011 |
| 75pct | 0.498 | -0.003 |
| none | 0.500 | +0.000 |

### Key Finding: NEGATIVE EFFECT
Disambiguation *decreases* H1 preference rather than increasing it.

### Interpretation
The H2 labels (position-based) don't represent a meaningful competing hypothesis.
For proper disambiguation effects, need two conflicting semantic clusterings:
- H1: Fine-grained clusters
- H2: Coarse superclusters (or another valid interpretation)

### Next Steps
1. Redesign H1/H2 to represent genuinely competing interpretations
2. Use dual-interpretation graph structure from velocity experiments
3. Test if disambiguation shifts preference between two valid hypotheses

---

## 2026-02-06: Preference with Dual Interpretation Graph (PROPER SETUP)

### Objective
Test cluster preference with two genuinely competing semantic interpretations, using the same dual-interpretation graph setup from velocity experiments.

### Key Improvement Over Previous Experiment
Instead of position-based H2 labels (meaningless), both H1 and H2 now represent **valid semantic clusterings** with ~60% agreement:
- **H1**: Cluster structure under graph G1
- **H2**: Cluster structure under graph G2
- Both are valid interpretations, creating genuine structural ambiguity

### Graph Statistics
| Metric | Value |
|--------|-------|
| Tokens | 15 |
| Clusters | 3 |
| G1 edges | 38 |
| G2 edges | 37 |
| Ambiguous edges | 14 |
| **Clustering agreement rate** | **0.60** |

### Setup
- Model: Qwen/Qwen2.5-7B
- Context length: 200 tokens
- N trials: 50
- Layers: 0, 7, 14, 21, 27
- Distance metric: Cosine similarity
- Window size: 20

### Results

| Condition | Disambig Pos | H1 Consistency | H2 Consistency | L27 Final Pref | L27 Change |
|-----------|--------------|----------------|----------------|----------------|------------|
| 25pct | 50 | 1.000 | 0.523 | 0.544 | **+0.060** |
| 50pct | 100 | 1.000 | 0.683 | 0.545 | **+0.107** |
| 75pct | 150 | 1.000 | 0.842 | 0.543 | **+0.080** |
| none | - | 1.000 | 1.000 | 0.434 | +0.000 |

### Key Findings

1. **POSITIVE PREFERENCE CHANGE**: Unlike the flawed experiment with position-based H2, disambiguation now **increases** H1 preference as expected.

2. **Strongest effect at 50% disambiguation**: The +0.107 change at layer 27 is the largest, suggesting mid-context disambiguation provides optimal signal-to-noise ratio.

3. **Baseline shows H2 preference**: The "none" condition shows 0.434 preference (slight H2 bias), indicating both hypotheses are equally valid in the fully ambiguous context.

4. **H2 consistency drops with earlier disambiguation**:
   - 25pct: H2 consistency = 0.523 (almost half the tokens invalid under H2)
   - 75pct: H2 consistency = 0.842 (more time in ambiguous region)

5. **Layer 27 consistently shows strongest effect**: Final layer has highest preference values and clearest differentiation between conditions.

### Interpretation
The proper dual-interpretation setup validates the original hypothesis: models encode structural information from in-context examples, and disambiguation shifts representations toward the correct interpretation. The 60% clustering agreement creates genuine ambiguity that gets resolved through context.

### Plots
- `results/preference_dual_interp/layer27_preference.png` - Preference trajectory at final layer
- `results/preference_dual_interp/preference_all_layers.png` - All layers comparison
- `results/preference_dual_interp/preference_change.png` - Pre/post disambiguation comparison

### W&B
[preference-dual-interp](https://wandb.ai/thomasjiralerspong/icl-structural-influence/runs/prr23b4v)

---

## 2026-02-09: Collapse Over Context - WildChat Long Conversations

### Objective
Extend collapse experiment with long, real user-assistant conversations from WildChat dataset to compare against short concatenated OpenAssistant conversations.

### Background
Previous `natural_conversation` condition used OpenAssistant/oasst1 dataset which contains many short conversations (~5970 user turns) concatenated together. This doesn't capture the dynamics of a single long conversation with one user.

WildChat (allenai/WildChat) contains real ChatGPT conversations with many having 10+ turns, providing better signal for long-context collapse.

### Dataset Details
- **Source**: allenai/WildChat
- **Filter**: Conversations with >= 10 turns
- **Found**: 1000 long conversations (from ~20K checked)
- **Format**: User/Assistant alternating turns

### Setup
- Model: Qwen/Qwen2.5-7B
- Context length: 10,000 tokens
- N trials: 5
- Layers: 0, 7, 14, 21, 27
- Window size: 50 tokens
- Checkpoints: 41 points from 10 to 10,000

### Code Changes
1. Added `load_wildchat_conversation()` method to `src/data/natural_language_loader.py`
2. Added `natural_wildchat` condition to collapse experiment
3. Updated plotting scripts to include WildChat in data type comparisons
4. Added `datasets` library to pyproject.toml dependencies

### Results
All 5 trials completed successfully with 10,000 tokens each.

Results merged into main collapse_10k_experiment for comparison with other natural language conditions:
- natural_books (Project Gutenberg)
- natural_conversation (OpenAssistant - short concatenated)
- natural_wildchat (WildChat - long single conversations)

### Key Plots
Updated data type comparison plots now include WildChat:
- `results/collapse_10k_experiment/plots/avg_cos_sim_data_type.png`
- `results/collapse_10k_experiment/plots/spread_data_type.png`
- `results/collapse_10k_experiment/plots/effective_dim_data_type.png`
- `results/collapse_10k_experiment/plots/intrinsic_dim_data_type.png`

All-layers comparison:
- `results/collapse_10k_experiment/plots/*_all_layers_data_type.png`

### Files Created
- `results/collapse_wildchat_experiment/` - Standalone WildChat results
- Raw trials: `results/collapse_10k_experiment/raw/natural_wildchat_trial_*.json`

---
