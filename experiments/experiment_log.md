# Experiment Log

This file tracks experiment results, meeting notes, and research directions.

---

## 2026-02-06: Meeting Notes

### Current Results Discussion
- Velocity experiments showing interesting disambiguation effects
- Early disambiguation (10%) shows strongest velocity drop (~12%)
- Effect builds progressively through layers, clearest in final 5-7 layers

### Methodological Considerations

**Metrics to revisit:**
- Curse of dimensionality may affect distance-based metrics
- **Action**: Use cosine similarity instead of Euclidean distance for cluster preference experiments
- Note: Some tokens have same connectivity patterns

**Open questions:**
- Do in-context representations change at the last layer?
- If only one hypothesis exists, what does velocity look like?
- Need to find better y-axis metrics to understand what's happening

### Research Directions

#### 1. Representational Collapse (PRIMARY FOCUS)
- Investigate when/why collapse happens
- Related concepts to explore:
  - "Echo chamber" effects
  - Can collapse be reverted?
- **Experiments to run:**
  - Random walk on one hypothesis ‚Üí observe collapse
  - Then random walk on other hypothesis ‚Üí does it recover?
  - Reproduce ring structure and check for collapse

#### 2. Performance Linkage
- Track if representational collapse correlates with task performance
- Important for practical implications

#### 3. Semantic Knowledge Conflicts
- Test if existing semantic knowledge affects collapse
- Hypothesis: Conflict level impacts representational collapse severity
- Current experiments use random token sampling ‚Üí high variance
- Need controlled experiments with varying conflict levels

#### 4. Natural Language Validation
- Test if velocity/collapse effects replicate with natural language prompts
- Important for generalization beyond synthetic graphs

#### 5. Pretraining vs Fine-tuning Dynamics
- Literature suggests: ICL can override SFT but not pretraining
- Interesting to show ICL can override hierarchical representations
- Compare: Pretraining vs SFT vs RLHF effects

### Context from Tommy
- "Context rot" is a known problem in practice
- Effective context length often only ~25% of stated length
- Our findings may relate to this degradation

### Literature Review Needed
- Representational collapse in LLMs
- Echo chamber effects in neural networks
- Context length degradation / context rot

### Immediate Next Steps
1. Look at representational collapse - when does it happen?
2. Find different y-axis metrics (beyond raw velocity)
3. Literature review on collapse phenomena

---

## 2026-02-06: Literature Review - Collapse Phenomena

### 1. Model Collapse (Training on Synthetic Data)

**Key Paper**: [AI models collapse when trained on recursively generated data](https://www.nature.com/articles/s41586-024-07566-y) (Nature, July 2024)

**Definition**: When models are trained on data generated by previous model versions, irreversible defects emerge where tails of the original distribution disappear.

**Stages**:
- **Early collapse**: Tails of distribution disappear (rare events vanish first)
- **Late collapse**: Model converges to shrunken distribution with very low variance

**Key findings**:
- Even 0.1% synthetic data can trigger collapse ([Strong Model Collapse, ICLR 2025](https://openreview.net/forum?id=et5l9qPUhm))
- Larger models can *amplify* collapse, not mitigate it
- By April 2025, ~30-40% of web corpus is synthetic
- Results in "knowledge collapse" - regression to mainstream/central tendencies

**Relevance to our work**: Our velocity collapse after disambiguation may relate to the model converging to a lower-variance representation state.

---

### 2. Neural Collapse (Classification Networks)

**Key Paper**: [Prevalence of neural collapse during terminal phase of training](https://www.pnas.org/doi/10.1073/pnas.2015509117) (PNAS)

**Definition**: Simplification in learned representations during late training stages (Terminal Phase of Training, TPT).

**Four manifestations (NC1-NC4)**:
1. **NC1 - Variability collapse**: Within-class variation becomes negligible, activations collapse to class means
2. **NC2 - Simplex ETF**: Class means converge to equal-length vectors with maximally equal angles
3. **NC3 - Self-duality**: Linear classifiers approach their corresponding class means
4. **NC4 - Simplified decisions**: Nearest-class-center becomes optimal classifier

**Benefits**: Convergence to this structure correlates with improved generalization and adversarial robustness.

**Extended findings**:
- [Neural collapse in intermediate layers](https://arxiv.org/abs/2308.02760) - some degree emerges in most hidden layers
- [Generalized neural collapse](https://arxiv.org/abs/2310.05351) - extends to cases where #classes >> feature dimension (relevant for LLMs)

**Relevance to our work**: Our late-layer velocity drop may indicate neural collapse-like convergence to simplified representations.

---

### 3. Context Rot / Lost in the Middle

**Key Paper**: [Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/abs/2307.03172) (Stanford, 2023)

**Definition**: Performance degradation as context length increases, even for simple tasks.

**Key findings** ([Chroma Research](https://research.trychroma.com/context-rot)):
- With 20 retrieved documents (~4K tokens), accuracy drops from 70-75% to 55-60%
- U-shaped attention bias: beginning and end tokens receive higher attention
- Effective context length often only ~25% of stated maximum
- [RoPE introduces long-term decay](https://arxiv.org/abs/2406.16008) that deprioritizes middle content

**Solutions proposed**:
- [Found in the Middle](https://arxiv.org/abs/2406.16008): Calibration mechanism for faithful attention
- Strategic document ordering (important info at start/end)
- Multi-scale Positional Encoding (Ms-PoE)

**Relevance to our work**: Our velocity patterns across context length may reflect attention distribution changes. The ~25% effective length aligns with Tommy's observation.

---

### 4. Representation Degeneration / Anisotropy

**Key Papers**:
- [Anisotropy Is Inherent to Self-Attention](https://arxiv.org/html/2401.12143v2)
- [Anisotropy is Not Inherent to Transformers](https://aclanthology.org/2024.naacl-long.274/) (NAACL 2024)

**Definition**: Hidden representations become unexpectedly close in angular distance (high cosine similarity), reducing expressiveness.

**Causes**:
- Long-tailed token distributions push representations toward specific directions
- Cross-entropy loss optimization on rare tokens
- Unused/rare tokens push all representations away from origin

**Architecture differences**:
- **Encoders**: Uniformly distributed anisotropy across layers
- **Decoders**: Bell-shaped curve with highest anisotropy in middle layers

**Training dynamics** ([Shape of Learning](https://arxiv.org/abs/2311.05928)):
- Intrinsic dimension increases early (expansion phase)
- Then decreases late in training (compression phase)

**Relevance to our work**: High-dimensional distance metrics may be affected. Cosine similarity recommended (aligns with meeting note).

---

### 5. Layer-wise Representation Dynamics

**Key Paper**: [Layer by Layer: Uncovering Hidden Representations](https://arxiv.org/html/2502.02013) (2025)

**Findings**:
- Early layers: High similarity across models
- Middle layers: Diverse, task-specific patterns
- Later layers: Convergence again (for prediction)

**Compression patterns**:
- Larger models show more pronounced intermediate compression (entropy dips)
- Performance often peaks in middle layers where compression preserves semantic details
- Smoother token trajectories in larger models

**Concept depth** ([Exploring Concept Depth](https://arxiv.org/html/2404.07066v1)):
- Emotional concepts: accuracy rises in initial layers, converges in intermediate
- Complex tasks: convergence at significantly deeper layers
- Different model families converge at different layer depths

**Relevance to our work**: Our layer-wise velocity patterns (effect building through layers 20-27) may reflect this convergence dynamic.

---

### 6. In-Context Learning Representation Theory

**Key Paper**: [In-Context Learning with Representations](https://arxiv.org/abs/2408.10147) (NeurIPS 2024)

**Findings**:
- Transformers learn to perform ridge regression over basis functions
- MLPs act as common nonlinear feature maps, enhancing ICL power
- Training loss converges linearly to global minimum
- First proof that transformers learn contextual information for generalization

**Dynamics**: Mean-field analysis shows loss landscape becomes benign, gradient flow avoids saddle points.

**Relevance to our work**: Theoretical grounding for how in-context examples shape representations.

---

### Summary Table: Collapse Phenomena

| Phenomenon | Domain | Mechanism | Reversible? | Our Connection |
|------------|--------|-----------|-------------|----------------|
| Model Collapse | Training | Synthetic data recursion | No (without real data) | Velocity reduction may be analogous |
| Neural Collapse | Classification | Late-stage training | N/A (training outcome) | Late-layer convergence patterns |
| Context Rot | Inference | Attention dilution | Partially (with calibration) | Context length effects |
| Anisotropy | Embeddings | Token distribution | Yes (with techniques) | Use cosine similarity |
| Layer Convergence | Deep networks | Compression for prediction | N/A (architectural) | Layer-wise velocity patterns |

---

### Open Questions for Our Research

1. **Is our velocity drop a form of "inference-time neural collapse"?**
   - Representations converging to simplified structure after disambiguation

2. **Can we measure anisotropy changes during context processing?**
   - Track isotropy/anisotropy as context grows

3. **Does our collapse correlate with context rot?**
   - Test if disambiguation helps or hurts downstream task performance

4. **Can collapse be reversed by conflicting context?**
   - After single-hypothesis collapse, introduce contradicting hypothesis

5. **How does this interact with the "lost in the middle" phenomenon?**
   - Is disambiguation position effect related to attention bias?

---

## Research Task: Anisotropy Tracking

### Status: IN PROGRESS (via collapse experiment)

### Objective
Measure how anisotropy changes as context length grows to understand if "context rot" is fundamentally an anisotropy problem.

### Connection to Current Work
Our `avg_cos_sim` metric in the collapse experiment directly measures anisotropy:
- `avg_cos_sim ‚Üí 1.0` = high anisotropy = representations collapsed to narrow cone
- `avg_cos_sim ‚Üí 0.0` = isotropic = healthy uniform spread

### Literature
See `docs/literature/anisotropy_references.md` for full references.

Key papers:
1. **Anisotropy Is Inherent to Self-Attention** (2024) - https://arxiv.org/abs/2401.12143
2. **Anisotropy is Not Inherent to Transformers** (NAACL 2024) - https://aclanthology.org/2024.naacl-long.274/
3. **Shape of Learning** (2023) - https://arxiv.org/abs/2311.05928

### Key Findings from Literature
- Decoders show bell-shaped anisotropy curve (highest in middle layers)
- Training dynamics: expansion phase ‚Üí compression phase
- Long-tailed token distributions push representations toward specific directions

### Analysis Plan (after collapse experiment completes)
1. Plot `avg_cos_sim` vs context length for all conditions
2. Compare anisotropy patterns: structured vs natural language vs vocabulary size
3. Check if disambiguation reduces anisotropy (restores isotropy)
4. Examine layer-wise anisotropy patterns (expect bell-shape in middle layers)

### Hypotheses
- H1: Anisotropy increases monotonically with context length (context rot)
- H2: Disambiguation temporarily reduces anisotropy before it increases again
- H3: Natural language shows different anisotropy trajectory than structured walks

---

## 2026-02-06: Cluster Preference Metric Comparison

### Objective
Compare distance metrics for cluster preference computation to address curse of dimensionality concerns.

### Metrics Tested
1. **L2 distance** (baseline) - Euclidean distance in full 3584-dim space
2. **Cosine similarity** - Angular distance, scale-invariant
3. **PCA + L2** - Dimensionality reduction to 10, 25, 50, 100 dims, then L2

### Results (50 trials, Qwen2.5-7B, 200 tokens)

| Metric | Layer 0 | Layer 27 | Variance Pattern |
|--------|---------|----------|------------------|
| L2 | 0.531 | 0.537 | High in mid-layers (0.075) |
| **Cosine** | **0.545** | **0.559** | **Consistently low (0.008-0.026)** |
| PCA-10 | 0.543 | 0.551 | Medium |
| PCA-25 | 0.535 | 0.542 | Medium |

### Key Findings
1. **Cosine similarity is recommended** - highest signal, lowest variance
2. **L2 suffers from curse of dimensionality** - high variance in middle layers
3. **PCA-10 is a good alternative** - nearly matches cosine with computational benefits
4. All metrics show H1 preference >0.5 (model encodes graph structure)
5. Layer 27 shows strongest cluster preference (final layer convergence)

### Implications
- Switch to cosine similarity for future cluster preference experiments
- The high L2 variance in layers 7-21 explains why previous experiments showed weak signals
- Low-dimensional PCA projection preserves discriminative structure

### Plots
- `results/cluster_preference_comparison/preference_by_metric.png`
- `results/cluster_preference_comparison/preference_heatmap.png`

---

## W&B Runs

| Date | Run Name | URL | Description |
|------|----------|-----|-------------|
| 2026-02-06 | velocity-scaled-20trials | [link](https://wandb.ai/thomasjiralerspong/icl-structural-influence/runs/17rtiyzh) | 20 trials, 28 layers, 1000 tokens, 8 disambiguation conditions |
| 2026-02-06 | cluster-preference-metric-comparison | [link](https://wandb.ai/thomasjiralerspong/icl-structural-influence/runs/y9te13mp) | 50 trials comparing L2/cosine/PCA metrics |
| 2026-02-06 | preference-disambiguation-cosine | [link](https://wandb.ai/thomasjiralerspong/icl-structural-influence/runs/akzijj9m) | 50 trials, disambiguation effects with cosine similarity |
| 2026-02-06 | preference-dual-interp | [link](https://wandb.ai/thomasjiralerspong/icl-structural-influence/runs/prr23b4v) | 50 trials, proper dual-interpretation graph with ~60% agreement |

---

## 2026-02-06: Preference Disambiguation Experiment

### Objective
Track how cluster preference (cosine similarity) evolves with disambiguation.

### Setup
- H1: Graph cluster labels (semantic structure)
- H2: Position-based labels (i % 3) - control/baseline
- Conditions: 25%, 50%, 75% disambiguation, and none

### Results

| Condition | Layer 27 Final Pref | Preference Change |
|-----------|--------------------|--------------------|
| 25pct | 0.481 | -0.019 |
| 50pct | 0.495 | -0.011 |
| 75pct | 0.498 | -0.003 |
| none | 0.500 | +0.000 |

### Key Finding: NEGATIVE EFFECT
Disambiguation *decreases* H1 preference rather than increasing it.

### Interpretation
The H2 labels (position-based) don't represent a meaningful competing hypothesis.
For proper disambiguation effects, need two conflicting semantic clusterings:
- H1: Fine-grained clusters
- H2: Coarse superclusters (or another valid interpretation)

### Next Steps
1. Redesign H1/H2 to represent genuinely competing interpretations
2. Use dual-interpretation graph structure from velocity experiments
3. Test if disambiguation shifts preference between two valid hypotheses

---

## 2026-02-06: Preference with Dual Interpretation Graph (PROPER SETUP)

### Objective
Test cluster preference with two genuinely competing semantic interpretations, using the same dual-interpretation graph setup from velocity experiments.

### Key Improvement Over Previous Experiment
Instead of position-based H2 labels (meaningless), both H1 and H2 now represent **valid semantic clusterings** with ~60% agreement:
- **H1**: Cluster structure under graph G1
- **H2**: Cluster structure under graph G2
- Both are valid interpretations, creating genuine structural ambiguity

### Graph Statistics
| Metric | Value |
|--------|-------|
| Tokens | 15 |
| Clusters | 3 |
| G1 edges | 38 |
| G2 edges | 37 |
| Ambiguous edges | 14 |
| **Clustering agreement rate** | **0.60** |

### Setup
- Model: Qwen/Qwen2.5-7B
- Context length: 200 tokens
- N trials: 50
- Layers: 0, 7, 14, 21, 27
- Distance metric: Cosine similarity
- Window size: 20

### Results

| Condition | Disambig Pos | H1 Consistency | H2 Consistency | L27 Final Pref | L27 Change |
|-----------|--------------|----------------|----------------|----------------|------------|
| 25pct | 50 | 1.000 | 0.523 | 0.544 | **+0.060** |
| 50pct | 100 | 1.000 | 0.683 | 0.545 | **+0.107** |
| 75pct | 150 | 1.000 | 0.842 | 0.543 | **+0.080** |
| none | - | 1.000 | 1.000 | 0.434 | +0.000 |

### Key Findings

1. **POSITIVE PREFERENCE CHANGE**: Unlike the flawed experiment with position-based H2, disambiguation now **increases** H1 preference as expected.

2. **Strongest effect at 50% disambiguation**: The +0.107 change at layer 27 is the largest, suggesting mid-context disambiguation provides optimal signal-to-noise ratio.

3. **Baseline shows H2 preference**: The "none" condition shows 0.434 preference (slight H2 bias), indicating both hypotheses are equally valid in the fully ambiguous context.

4. **H2 consistency drops with earlier disambiguation**:
   - 25pct: H2 consistency = 0.523 (almost half the tokens invalid under H2)
   - 75pct: H2 consistency = 0.842 (more time in ambiguous region)

5. **Layer 27 consistently shows strongest effect**: Final layer has highest preference values and clearest differentiation between conditions.

### Interpretation
The proper dual-interpretation setup validates the original hypothesis: models encode structural information from in-context examples, and disambiguation shifts representations toward the correct interpretation. The 60% clustering agreement creates genuine ambiguity that gets resolved through context.

### Plots
- `results/preference_dual_interp/layer27_preference.png` - Preference trajectory at final layer
- `results/preference_dual_interp/preference_all_layers.png` - All layers comparison
- `results/preference_dual_interp/preference_change.png` - Pre/post disambiguation comparison

### W&B
[preference-dual-interp](https://wandb.ai/thomasjiralerspong/icl-structural-influence/runs/prr23b4v)

---

## 2026-02-09: Collapse Reversal Experiment

### Objective
Test whether representational collapse can be reversed by injecting contradicting information after a non-ambiguous sequence has caused collapse.

### Setup
- Model: Qwen/Qwen2.5-7B
- Phase 1: 5000 tokens (H1-only walk to induce collapse)
- Phase 2: 5000 tokens (injection of contradicting information)
- Total: 10,000 tokens per trial
- Layers: 0, 7, 14, 21, 27
- N trials: 5 per condition
- Window size: 50

### Conditions Tested
1. **Control (H1 continuous)**: No injection, continue H1 walk
2. **H2 Injection (same tokens)**: Switch to H2-only walk using same vocabulary
3. **Different Graph**: Switch to H1 walk on completely different graph/vocabulary
4. **Natural Books**: Switch to natural language from Project Gutenberg
5. **Natural Wikipedia**: Switch to natural language from Wikipedia

### Key Results

**COLLAPSE IS PARTIALLY REVERSIBLE**

| Injection Type | Reversal Strength | Cos Sim Change (L14) | Mechanism |
|----------------|-------------------|----------------------|-----------|
| Natural Books | **Strong** | -0.30 (0.65‚Üí0.35) | Complete token distribution shift |
| Natural Wikipedia | Moderate | -0.15 | Token distribution shift |
| Different Graph | Moderate | -0.05 | New tokens with new statistics |
| H2 Same Tokens | **Weak** | -0.01 | Same tokens, structure change ignored |
| Control | None | 0.00 | Continued collapse |

### Major Findings

1. **Natural language causes strongest reversal**: Books and Wikipedia injection produce dramatic drops in cosine similarity, indicating sustained representational diversity.

2. **Token identity > latent structure**: H2 injection (same tokens, different structure) has minimal effect, suggesting collapse is primarily driven by token-level patterns, not structural relationships.

3. **Layer-dependent effects**:
   - Layers 0, 7, 14: Natural language and different graph increase effective dimension
   - Layer 27: Inverted effect - natural language *decreases* effective dimension

4. **Immediate vs sustained effects**: Different graph shows immediate spike at injection but settles back; natural language maintains lower cosine similarity throughout Phase 2.

### Plots
- `results/collapse_reversal/plots/trajectory_avg_cos_sim.png`
- `results/collapse_reversal/plots/trajectory_effective_dim.png`
- `results/collapse_reversal/plots/delta_avg_cos_sim.png`
- `results/collapse_reversal/plots/delta_effective_dim.png`
- `results/collapse_reversal/plots/before_after_comparison.png`

### Summary Document
See `experiments/2026-02-09_collapse-reversal.md` for full analysis.

### Follow-up Questions
1. Does reversal persist if we return to H1 after injection? (Phase 3 experiment)
2. What is minimum natural language injection needed to cause reversal?
3. Can targeted token injection cause reversal without full natural language?

---

## 2026-02-09: Collapse Over Context - WildChat Long Conversations

### Objective
Extend collapse experiment with long, real user-assistant conversations from WildChat dataset to compare against short concatenated OpenAssistant conversations.

### Background
Previous `natural_conversation` condition used OpenAssistant/oasst1 dataset which contains many short conversations (~5970 user turns) concatenated together. This doesn't capture the dynamics of a single long conversation with one user.

WildChat (allenai/WildChat) contains real ChatGPT conversations with many having 10+ turns, providing better signal for long-context collapse.

### Dataset Details
- **Source**: allenai/WildChat
- **Filter**: Conversations with >= 10 turns
- **Found**: 1000 long conversations (from ~20K checked)
- **Format**: User/Assistant alternating turns

### Setup
- Model: Qwen/Qwen2.5-7B
- Context length: 10,000 tokens
- N trials: 5
- Layers: 0, 7, 14, 21, 27
- Window size: 50 tokens
- Checkpoints: 41 points from 10 to 10,000

### Code Changes
1. Added `load_wildchat_conversation()` method to `src/data/natural_language_loader.py`
2. Added `natural_wildchat` condition to collapse experiment
3. Updated plotting scripts to include WildChat in data type comparisons
4. Added `datasets` library to pyproject.toml dependencies

### Results
All 5 trials completed successfully with 10,000 tokens each.

Results merged into main collapse_10k_experiment for comparison with other natural language conditions:
- natural_books (Project Gutenberg)
- natural_conversation (OpenAssistant - short concatenated)
- natural_wildchat (WildChat - long single conversations)

### Key Plots
Updated data type comparison plots now include WildChat:
- `results/collapse_10k_experiment/plots/avg_cos_sim_data_type.png`
- `results/collapse_10k_experiment/plots/spread_data_type.png`
- `results/collapse_10k_experiment/plots/effective_dim_data_type.png`
- `results/collapse_10k_experiment/plots/intrinsic_dim_data_type.png`

All-layers comparison:
- `results/collapse_10k_experiment/plots/*_all_layers_data_type.png`

### Files Created
- `results/collapse_wildchat_experiment/` - Standalone WildChat results
- Raw trials: `results/collapse_10k_experiment/raw/natural_wildchat_trial_*.json`

---

## 2026-02-12: New Research Tasks

### Task 1: LLM Failure Mode Collapse Experiments

Test representation collapse during known LLM failure modes:

1. **Reasoning loops** - Problems where models get stuck in circular reasoning
2. **Seahorse emoji** (ü¶à/üê¥) - Known tokenization/generation failure
3. **Other failure modes** - Literature review needed: repetition collapse, degenerate outputs, hallucination spirals

**Goal**: Do representations collapse before/during these failures? Can we detect failure onset from representation geometry?

### Task 2: Longer Conversations

Extend context length experiments beyond current 10K tokens:
- Push to 32K+ tokens where models start degrading
- Track all 5 collapse metrics over very long contexts
- Compare degradation patterns across models with different context windows

### Task 3: Attractor Dynamics

Investigate whether representational collapse reflects attractor dynamics:

1. **Natural setting attractors** - Are there representation states where the model gets "trapped"?
2. **Perturbation recovery** - After the model enters an attractor state, how does perturbation (e.g., injection of different content) help escape?
3. **Basin of attraction** - How large is the perturbation needed to escape? Does this vary by layer?
4. **Connection to collapse reversal** - Extends the Feb 9 reversal experiment with dynamical systems framing

### Task 4: Representation Changes After Long Context

**Core question**: How do final representations change as context grows?

**Representations to track**:
- Final token representation
- Mean representation across all tokens

**Context types** (long context conditions):
- WildChat (natural long conversations)
- Many-shot jailbreak sequences
- Persona drift conversations
- One repeated token (degenerate baseline)

**Probes to apply after context**:
- Knowledge questions (factual recall)
- Reasoning questions (multi-step inference)

**Key research question**: Does the model use context or its weights to retrieve knowledge, and how does this vary with context length?
- At short context: model likely uses weights (parametric knowledge)
- At long context: does it shift to using context? Or does context corrupt weight-based retrieval?
- Measure by comparing accuracy on knowledge/reasoning questions with and without relevant context

### Task 5: SpiralBench

Try SpiralBench dataset for evaluating LLM failure modes related to reasoning spirals and loops. Investigate whether representation collapse correlates with spiral/loop onset in this benchmark.

---

## 2026-02-12: Reasoning Loop Collapse Experiment

### Experiment: Representation Collapse During Reasoning Tasks

**Model**: Qwen/Qwen2.5-7B-Instruct
**Script**: `experiments/core/run_reasoning_loop_collapse_experiment.py`
**Results**: `results/reasoning_loop_collapse/`

### Objective
Test whether representations collapse during reasoning tasks that are known to induce circular reasoning loops in LLMs (based on LoopBench categories).

### Configuration
- 15 prompts across 5 categories: arithmetic (4), recursive (3), logic (3), enumeration (2), control (3)
- Layers: [0, 7, 14, 21, 27]
- Window size: 30, checkpoint every 5 positions
- Greedy decoding (do_sample=False)

### Key Finding: Qwen2.5-7B-Instruct Did NOT Loop

None of the 15 prompts induced actual loops. The model handles all reasoning tasks cleanly, including:
- Tower of Hanoi (7 disks, 1959 tokens)
- Long division (1024 tokens)
- Fibonacci sequence (1500 tokens)
- Liar paradox

### Collapse Patterns (No Loops, but Informative)

| Category | Avg CosSim (L27) | Avg EffDim (L27) |
|----------|-------------------|-------------------|
| Arithmetic | 0.535 | 14.3 |
| Enumeration | 0.418 | 13.9 |
| Recursive | 0.373 | 12.3 |
| Logic | 0.382 | 11.8 |
| Control | 0.304 | 16.1 |

**Key observations:**
1. **Arithmetic tasks show highest collapse** (cos_sim 0.46-0.66) - repetitive numerical patterns
2. **sqrt_manual shows most collapse** (cos_sim=0.664) - iterative calculation is highly structured
3. **Controls show lowest collapse** (cos_sim=0.20-0.38) - diverse content prevents convergence
4. **Truth-teller puzzle has lowest eff_dim** (8.4) - reasoning concentrates on few dimensions
5. **No loops = no collapse** in the dramatic sense seen with seahorse emoji (cos_sim=0.99)

### Performance Fix: Gram Matrix Trick

Fixed critical performance bug in `src/metrics/collapse_metrics.py`:
- `np.cov(reps.T)` created 3584√ó3584 covariance matrix for eigendecomposition
- `np.linalg.eigvalsh` on this matrix: O(3584¬≥) = ~46 billion ops per call
- With 1000 checkpoints per experiment: caused hours of CPU compute at 3500% utilization
- **Fix**: When n_samples < n_dims, use Gram matrix (30√ó30) instead. Same eigenvalues, O(30¬≥) = trivial
- Also vectorized L2 distance computation using scipy pdist

### Next Steps
- Need to find models that actually DO loop (smaller models, base models without instruct tuning)
- Or use longer generation (5000+ tokens) to exhaust the model
- Or try adversarial prompts (OverThink-style decoy injection)

### Follow-up: Qwen2.5-7B Base Model (Same Prompts)

**Model**: Qwen/Qwen2.5-7B (base, no instruct tuning)
**Results**: `results/reasoning_loop_collapse_qwen25_base/`

Tested same 15 prompts on base model to compare with instruct version.

**Result: Base model also did NOT loop**, but showed different collapse patterns:

| Prompt | Category | Tokens | CosSim (L27) | EffDim (L27) |
|--------|----------|--------|--------------|--------------|
| permutations | enumeration | 2000 | **0.669** | **4.4** |
| self_reference_loop | logic | 173 | 0.609 | 12.2 |
| long_division_hard | arithmetic | 476 | 0.587 | 11.1 |
| control_simple_math | control | 133 | 0.567 | 8.1 |
| hanoi_large | recursive | 344 | 0.536 | 15.2 |
| hanoi_explain | recursive | 620 | 0.532 | 9.9 |
| counting_backwards | recursive | 71 | 0.522 | 12.0 |
| newton_iteration | arithmetic | 238 | 0.513 | 10.0 |
| sqrt_manual | arithmetic | 296 | 0.511 | 14.4 |
| list_primes | enumeration | 305 | 0.503 | 11.2 |
| fibonacci_manual | arithmetic | 288 | 0.473 | 16.6 |
| liar_paradox | logic | 134 | 0.415 | 14.6 |
| control_explanation | control | 98 | 0.362 | 16.7 |
| truth_teller_puzzle | logic | 1024 | 0.348 | 9.9 |
| control_short_story | control | 499 | 0.315 | 17.4 |

**Instruct vs Base Comparison:**
- Base model has **higher baseline collapse** even in controls (base: 0.315-0.567 vs instruct: 0.200-0.380)
- Base model generates shorter outputs overall (no instruct training to sustain reasoning)
- `permutations` is standout: 2000 tokens listing all 720 permutations, achieves near-seahorse-level collapse (cos_sim=0.669, eff_dim=4.4)
- No Llama models were available (no HF token for gated models)

---

## 2026-02-13: Repetition Collapse Experiment

### Experiment: Representation Collapse Across Repetition Types

**Model**: Qwen/Qwen2.5-7B-Instruct
**Script**: `experiments/core/run_repetition_collapse_experiment.py`
**Results**: `results/repetition_collapse_qwen25_instruct/`

### Objective
Systematically test how different types of repetition affect representational collapse, informed by:
- Repeat Curse (ACL 2025, arXiv:2504.14218) - SAE features causing repetition
- Induction Head Toxicity (arXiv:2505.13514) - ICL mechanism driving repetition
- "Repetitions Are Not All Alike" (arXiv:2504.01100) - natural vs ICL-induced mechanisms

### Configuration
- 20 prompts across 6 categories
- Layers: [0, 7, 14, 21, 27]
- Window size: 30, checkpoint every 5 positions
- Greedy decoding (do_sample=False)
- Also computes 1-gram/2-gram repetition scores and token entropy (from Repeat Curse)

### Key Findings

**Category averages (Layer 27):**

| Category | Avg CosSim | Avg EffDim | Loop Rate |
|----------|-----------|-----------|-----------|
| token_repetition | **0.440** | **6.3** | 4/5 |
| forced_copy | 0.408 | 10.1 | 2/2 |
| natural_greedy | 0.330 | 13.7 | 0/3 |
| control | 0.338 | 14.0 | 0/3 |
| enumeration | 0.325 | 11.5 | 0/4 |
| paragraph_repetition | 0.301 | 14.9 | 0/3 |

**Prompt-level highlights:**

| Prompt | Category | Tokens | Loop | CosSim | EffDim | RepScore |
|--------|----------|--------|------|--------|--------|----------|
| repeat_number_sequence | token_rep | 512 | YES | **0.693** | **2.9** | 1.000 |
| repeat_word_50 | token_rep | 512 | YES | **0.672** | **1.0** | 1.000 |
| copy_counting | forced_copy | 1024 | YES | 0.452 | 10.1 | 1.000 |
| enumerate_animals | enumeration | 1326 | no | 0.421 | 12.4 | 0.998 |
| enumerate_colors | enumeration | 1335 | no | 0.399 | 13.9 | 0.999 |
| control_diverse_questions | control | 131 | no | 0.314 | 16.7 | 0.729 |

### Key Observations

1. **Token repetition causes strongest collapse**: repeat_word_50 reaches cos_sim=0.672 and eff_dim=1.0 (single-dimensional collapse!). repeat_number_sequence reaches cos_sim=0.693, eff_dim=2.9.

2. **Instruct model resists some repetition**: repeat_word_200 refused to actually repeat 200 times, instead summarizing ("I will repeat... Here is the beginning and end..."). Only generated 38 tokens. This safety behavior prevents collapse.

3. **Paragraph repetition does NOT cause collapse**: Despite feeding 3x repeated paragraphs, the model generates diverse continuations. Average eff_dim=14.9 (highest of all categories!). The model effectively "reads through" repetitive context.

4. **Forced copy with structure loops**: copy_counting (1-20 repeated 10 times) loops at statement level but collapse is moderate (cos_sim=0.452) because the counting creates structural diversity within each cycle.

5. **Enumeration has high rep_score but low collapse**: enumerate_animals has rep_score=0.998 but cos_sim=0.421 and eff_dim=12.4. The shared structure ("1. X\n2. Y\n...") creates token-level repetition but semantic diversity prevents geometric collapse.

6. **Natural greedy generation does NOT spontaneously loop**: All 3 natural prompts (recipe, essay, instructions) generated 950-1600 tokens without looping. Instruct models are well-trained against degenerate generation.

### Comparison with Previous Experiments

| Condition | CosSim Range | EffDim Range | Source |
|-----------|-------------|-------------|--------|
| Seahorse emoji loops | 0.54-0.99 | 1.0-3.0 | seahorse experiment |
| Token repetition loops | 0.35-0.69 | 1.0-6.7 | **this experiment** |
| Structured arithmetic | 0.46-0.66 | 11-16 | reasoning experiment |
| Enumeration (no loop) | 0.15-0.42 | 5.9-13.9 | **this experiment** |
| Controls | 0.31-0.38 | 11.9-16.7 | **this experiment** |

### Collapse Hierarchy (from most to least):
1. **Degenerate emoji loops** (seahorse): cos_sim > 0.9, eff_dim ~ 1 [catastrophic]
2. **Token repetition loops**: cos_sim 0.35-0.69, eff_dim 1-7 [severe]
3. **Structured arithmetic**: cos_sim 0.46-0.66, eff_dim 11-16 [moderate]
4. **Forced copy loops**: cos_sim ~0.45, eff_dim ~10 [moderate]
5. **Enumeration**: cos_sim 0.15-0.42, eff_dim 6-14 [mild]
6. **Natural generation/controls**: cos_sim 0.30-0.38, eff_dim 12-17 [baseline]
7. **Paragraph repetition context**: cos_sim 0.20-0.40, eff_dim 15 [no effect]

---

## 2026-02-13: Attractor Dynamics Analysis

### Experiment: Characterizing Representational Attractors

**Script**: `experiments/core/run_attractor_dynamics_experiment.py`
**Results**: `results/attractor_dynamics/`
**Data Sources**: Collapse reversal (Feb 9-10), seahorse emoji, reasoning loops, repetition collapse

### Objective
Frame representational collapse as attractor dynamics in a dynamical systems sense. Synthesize all collapse experiments to identify: attractor types, basin sizes, escape conditions, and layer-wise formation.

### Key Findings

#### 1. Three Attractor Types Identified

| Attractor Type | eff_dim | Examples | Count |
|---------------|---------|----------|-------|
| **Point** (< 2) | ~1.0 | Emoji loops, repeat_word_50 | 4/57 |
| **Line** (2-5) | ~3 | repeat_number, permutations | 3/57 |
| **Manifold** (> 5) | 8-17 | Most generation, controls | 50/57 |

Point attractors are rare but extreme ‚Äî only occur during true degenerate loops.

#### 2. Attractor Escape Conditions (from reversal experiment)

| Perturbation | Immediate Delta | Final State | Escaped? |
|-------------|----------------|-------------|----------|
| Control (no injection) | -0.002 | 0.954 | N/A |
| H2 same vocabulary | -0.006 | 0.966 | **NO** |
| Different graph vocabulary | -0.051 | 0.952 | **NO** |
| Natural books | **-0.169** | **0.402** | **YES** |
| Natural Wikipedia | **-0.098** | **0.488** | **YES** |

**Critical finding**: Basin of attraction is defined primarily by **vocabulary repetition**, not structure. Same tokens with different graph structure cannot escape. Only domain shift (natural language) breaks the attractor.

#### 3. Recovery Dynamics

- H2 injection: Re-collapses within 10 tokens (immediate)
- Different graph: Transient disruption, recovers to 0.97 within 50 tokens
- Natural language: **Never recovers** ‚Äî stays at 0.40-0.49 for remaining 5000 tokens
- This implies the structured walk attractor has a well-defined basin boundary that natural text crosses but same-domain text does not

#### 4. Layer-wise Attractor Formation

From reversal data, the attractor is strongest at Layer 27 (cos_sim=0.97) and progressively weaker at earlier layers. When natural language breaks the attractor:
- Layer 27 drops from 0.97 to 0.80 immediately, then to 0.40
- Layer 0 barely affected (already low cos_sim)
- This suggests the attractor is primarily a **late-layer phenomenon**

### Plots Generated
- `results/attractor_dynamics/attractor_escape_landscape.png` - Before/after injection comparison
- `results/attractor_dynamics/attractor_taxonomy.png` - Scatter of all experiments by attractor type
- `results/attractor_dynamics/layerwise_attractor_formation.png` - Layer-wise trajectories

---

## 2026-02-13: 32K Context Collapse Experiment

**Model**: Qwen/Qwen2.5-7B (base)
**Conditions**: structured_no_ambig, structured_full_ambig, natural_books, natural_conversation
**Trials**: 3 per condition (12 total)
**Context length**: 32,000 tokens with 23 checkpoints (100 to 32000)
**Layers**: 0, 7, 14, 21, 27
**Window size**: 50

### Bug Fix
Fixed `src/metrics/collapse_metrics.py:200`: `if l2_dists:` ‚Üí `if len(l2_dists) > 0:` on numpy array from `pdist()`. This caused "truth value of array is ambiguous" error at every checkpoint when `compute_diagnostics=True`.

### Key Results

**Finding 1: Collapse is CONTENT-DEPENDENT, not universal**
- Structured walks: Early/mid layers collapse dramatically at 32K
- Natural language: NO collapse at ANY layer, even at 32K tokens

**Finding 2: Layer-dependent collapse dynamics for structured content**
| Layer | structured_no_ambig | structured_full_ambig | natural_books | natural_convo |
|-------|--------------------|-----------------------|---------------|---------------|
| L0  | dim 12.7‚Üí8.3 (mild) | dim 8.5‚Üí6.1 (mild) | dim 12.7‚Üí29.1 (expand) | dim 31.6‚Üí28.3 (stable) |
| L7  | dim 14.3‚Üí**1.2** (collapse!) | dim 10.6‚Üí3.2 | dim 14.4‚Üí26.8 (expand) | dim 25.7‚Üí25.3 (stable) |
| L14 | dim 14.7‚Üí**1.4** (collapse!) | dim 12.4‚Üí3.5 | dim 13.1‚Üí23.4 (expand) | dim 21.1‚Üí21.2 (stable) |
| L21 | dim 13.7‚Üí4.0 (compress) | dim 11.3‚Üí6.9 | dim 13.7‚Üí23.5 (expand) | dim 19.3‚Üí21.3 (stable) |
| L27 | dim 8.7‚Üí5.9 (mild) | dim 6.2‚Üí4.7 (mild) | dim 6.9‚Üí17.0 (expand) | dim 15.2‚Üí14.9 (stable) |

**Finding 3: Structured content collapse is worst in early/middle layers**
- Layers 7 and 14 collapse to effective dimension ~1 (single-dimensional!)
- Layer 27 barely compresses (dim 8.7‚Üí5.9)
- This is the OPPOSITE of the 10K generation experiments where collapse was strongest at L27

**Finding 4: Ambiguity provides partial protection**
- No-ambig collapses harder than full-ambig (L7: dim 1.2 vs 3.2)
- Ambiguous walks maintain slightly higher dimensionality throughout

**Finding 5: Natural language is inherently diverse**
- Books: eff_dim 20-30 across all layers, stable to 32K
- Conversations: eff_dim 15-25, completely stable
- Cosine similarity stays LOW (0.25-0.49) for natural language

### Interpretation
The 10K experiments (using model-generated output) showed collapse increasing at later layers because generation creates self-reinforcing loops. The 32K experiment uses pre-generated input tokens (no generation loop), so we see the raw representational dynamics:
- Repetitive structured vocabulary ‚Üí early/mid layer collapse
- Diverse natural language ‚Üí no collapse at any scale
- The generation loop (from prior experiments) amplifies collapse at later layers

### Plots Generated
- `results/collapse_32k_experiment/collapse_32k_effective_dim.png` - Eff dim trajectory by layer
- `results/collapse_32k_experiment/collapse_32k_cos_sim.png` - Cosine similarity trajectory by layer
- `results/collapse_32k_experiment/collapse_32k_l2_dist.png` - L2 distance trajectory by layer
- `results/collapse_32k_experiment/collapse_32k_heatmap_dim.png` - Effective dim heatmap
- `results/collapse_32k_experiment/collapse_32k_heatmap_cos.png` - Cosine similarity heatmap
- `results/collapse_32k_experiment/collapse_32k_summary_bars.png` - Final metrics comparison

---

## 2026-02-17: Probing Collapse-Performance Link

**Model**: Qwen/Qwen2.5-7B-Instruct
**Writeup**: `experiments/2026-02-17_probing-collapse-performance.md`

Tests whether representational collapse actually impairs knowledge retrieval.

### Three Experiments

**1. Original (2,580 evaluations)**: 4 context types x 7 lengths x 60 questions
- Structured walk degrades: 97% (500) ‚Üí 10% (20K)
- Natural books stable: ~97% throughout
- Repeated token fastest degradation: 87% (500) ‚Üí 0% (20K)
- Correlation collapse vs log-prob: r = -0.324

**2. Ignore instruction (1,020 evaluations)**: Raw ignore tokens injected after context
- Helps at moderate lengths: +46.7% for repeated_token at 5K
- Ineffective at 20K; slightly counterproductive for structured_walk

**3. Chat template (1,980 evaluations)**: Context wrapped in proper ChatML format
- Structured walk benefits significantly at 20K: 10% ‚Üí 26.7%
- Chat + ignore is best overall strategy for structured walk (95.6% at 5K, 82.8% at 10K)
- Repeated token: chat template actually *hurts* at 5K (8.3% ‚Üí 0%)
- Key insight: formatting helps structured content but not degenerate content

### Conclusions
- Collapse IS functionally harmful (not epiphenomenal)
- Content-type and formatting interact: no single mitigation works universally
- The 20K wall remains for all strategies (~27% max for structured walk)
- Attention flooding hypothesis supported: collapsed KV entries dominate attention

### Results
- `results/probing_collapse_performance/` - original
- `results/probing_collapse_ignore/` - ignore instruction
- `results/probing_collapse_chat/` - chat template
- Plots in `results/probing_collapse_performance/plots/`

---
